{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment_10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOYW0LZmuuiTdhcQ0aNNGeG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nh7881/machine_learning_project_class/blob/master/exercise_10/assignment_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFlnw0njY-Z2"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import transforms, datasets\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4o-wcNbbUcf"
      },
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,),(0.3081,)),  # mean value = 0.1307, standard deviation value = 0.3081\n",
        "])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8AuT_FfbYW7"
      },
      "source": [
        "data_path = './MNIST'\n",
        "minibatch_size=64\n",
        "\n",
        "training_set = datasets.MNIST(root = data_path, train= False, download=True, transform= transform)\n",
        "testing_set = datasets.MNIST(root = data_path, train= True, download=True, transform= transform)\n",
        "training_loader = torch.utils.data.DataLoader(training_set,minibatch_size)\n",
        "testing_loader = torch.utils.data.DataLoader(testing_set,minibatch_size)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ehu9i_-hbvLU",
        "outputId": "fd9fd0e5-5efc-444e-db6d-a70c599b3e66"
      },
      "source": [
        "print(training_loader.dataset.data.shape)\n",
        "print(testing_loader.dataset.data.shape)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10000, 28, 28])\n",
            "torch.Size([60000, 28, 28])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMZvU1iDbvpD"
      },
      "source": [
        "class classification(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(classification, self).__init__()\n",
        "        \n",
        "        # construct layers for a neural network\n",
        "        self.classifier1 = nn.Sequential(\n",
        "            nn.Linear(in_features=28*28, out_features=20*20),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout2d(0.2),\n",
        "        ) \n",
        "        self.classifier2 = nn.Sequential(\n",
        "            nn.Linear(in_features=20*20, out_features=10*10),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout2d(0.2),\n",
        "        ) \n",
        "        self.classifier3 = nn.Sequential(\n",
        "            nn.Linear(in_features=10*10, out_features=10),\n",
        "            nn.LogSoftmax(dim=1),\n",
        "        ) \n",
        "        \n",
        "        \n",
        "    def forward(self, inputs):                 # [batchSize, 1, 28, 28]\n",
        "        x = inputs.view(inputs.size(0), -1)    # [batchSize, 28*28]\n",
        "        x = self.classifier1(x)                # [batchSize, 20*20]\n",
        "        x = self.classifier2(x)                # [batchSize, 10*10]\n",
        "        out = self.classifier3(x)              # [batchSize, 10]\n",
        "        \n",
        "        return out\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppBNwEV6cpE6"
      },
      "source": [
        "criterion = nn.NLLLoss()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DChy4rNcczBe"
      },
      "source": [
        "learning_rate_value = 0.01\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "classifier = classification().to(device)\n",
        "optimizer = torch.optim.SGD(classifier.parameters(), lr=learning_rate_value, momentum=0.9)\n",
        "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.9)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxbWrFsBc0hk"
      },
      "source": [
        "test_losses = []\n",
        "test_accuracies = []\n",
        "def test() :\n",
        "    classifier.eval()\n",
        "    correct = 0\n",
        "    n = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(testing_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = classifier(data)\n",
        "            loss = criterion(output, target)\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "            n += len(data)\n",
        "        print('Test Loss: {:.6f} Correct: {}, ACC : {}'.format(\n",
        "            loss.item(), correct, correct/n))\n",
        "        test_losses.append(loss.item())\n",
        "        test_accuracies.append(correct/n)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPx5vcFsdKQ8",
        "outputId": "1c3bbaf2-e097-4c4a-afc4-f63876510d9b"
      },
      "source": [
        "epoch_num = 30\n",
        "losses = []\n",
        "accuracies = []\n",
        "for epoch in range(epoch_num) :\n",
        "    correct = 0\n",
        "    n = 0\n",
        "    classifier.train()\n",
        "    #scheduler.step()\n",
        "    for batch_idx, (data, target) in enumerate(training_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = classifier(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        pred = output.argmax(dim=1, keepdim=True)\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "        n += len(data)\n",
        "    print('Train Epoch: {} Loss: {:.6f} Correct: {} ACC: {}'.format(\n",
        "        epoch, loss.item(), correct, correct/n))\n",
        "    test()\n",
        "    losses.append(loss.item())\n",
        "    accuracies.append(correct/n)\n",
        "    #train  test    batch_size  loss    activ\n",
        "    #94.7,  91.95   16          ce      sigmoid\n",
        "    #?      93.7    64          ce      sigmoid\n",
        "    #97     92.8    64          nll     sigmoid\n",
        "    #100    94.9    16          nll     sigmoid\n",
        "    #100    95.5    64          nll     relu\n",
        "    #100    95.5    64          nll     relu    dropout추가"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 Loss: 0.292078 Correct: 7412 ACC: 0.7412\n",
            "Test Loss: 0.151410 Correct: 52036, ACC : 0.8672666666666666\n",
            "Train Epoch: 1 Loss: 0.081110 Correct: 9026 ACC: 0.9026\n",
            "Test Loss: 0.085904 Correct: 54441, ACC : 0.90735\n",
            "Train Epoch: 2 Loss: 0.050230 Correct: 9266 ACC: 0.9266\n",
            "Test Loss: 0.040112 Correct: 55515, ACC : 0.92525\n",
            "Train Epoch: 3 Loss: 0.014734 Correct: 9455 ACC: 0.9455\n",
            "Test Loss: 0.014855 Correct: 55882, ACC : 0.9313666666666667\n",
            "Train Epoch: 4 Loss: 0.022258 Correct: 9563 ACC: 0.9563\n",
            "Test Loss: 0.009833 Correct: 56339, ACC : 0.9389833333333333\n",
            "Train Epoch: 5 Loss: 0.011892 Correct: 9632 ACC: 0.9632\n",
            "Test Loss: 0.007022 Correct: 56535, ACC : 0.94225\n",
            "Train Epoch: 6 Loss: 0.003576 Correct: 9720 ACC: 0.972\n",
            "Test Loss: 0.007344 Correct: 56709, ACC : 0.94515\n",
            "Train Epoch: 7 Loss: 0.002484 Correct: 9764 ACC: 0.9764\n",
            "Test Loss: 0.004359 Correct: 56922, ACC : 0.9487\n",
            "Train Epoch: 8 Loss: 0.001277 Correct: 9819 ACC: 0.9819\n",
            "Test Loss: 0.003604 Correct: 57042, ACC : 0.9507\n",
            "Train Epoch: 9 Loss: 0.000899 Correct: 9843 ACC: 0.9843\n",
            "Test Loss: 0.004242 Correct: 57052, ACC : 0.9508666666666666\n",
            "Train Epoch: 10 Loss: 0.000865 Correct: 9865 ACC: 0.9865\n",
            "Test Loss: 0.002865 Correct: 57052, ACC : 0.9508666666666666\n",
            "Train Epoch: 11 Loss: 0.001110 Correct: 9895 ACC: 0.9895\n",
            "Test Loss: 0.001180 Correct: 57138, ACC : 0.9523\n",
            "Train Epoch: 12 Loss: 0.001369 Correct: 9913 ACC: 0.9913\n",
            "Test Loss: 0.000993 Correct: 57312, ACC : 0.9552\n",
            "Train Epoch: 13 Loss: 0.000369 Correct: 9938 ACC: 0.9938\n",
            "Test Loss: 0.001836 Correct: 57289, ACC : 0.9548166666666666\n",
            "Train Epoch: 14 Loss: 0.004784 Correct: 9936 ACC: 0.9936\n",
            "Test Loss: 0.000893 Correct: 57288, ACC : 0.9548\n",
            "Train Epoch: 15 Loss: 0.001409 Correct: 9934 ACC: 0.9934\n",
            "Test Loss: 0.000753 Correct: 57274, ACC : 0.9545666666666667\n",
            "Train Epoch: 16 Loss: 0.000545 Correct: 9955 ACC: 0.9955\n",
            "Test Loss: 0.000944 Correct: 57428, ACC : 0.9571333333333333\n",
            "Train Epoch: 17 Loss: 0.000174 Correct: 9965 ACC: 0.9965\n",
            "Test Loss: 0.000600 Correct: 57412, ACC : 0.9568666666666666\n",
            "Train Epoch: 18 Loss: 0.001746 Correct: 9977 ACC: 0.9977\n",
            "Test Loss: 0.000622 Correct: 57412, ACC : 0.9568666666666666\n",
            "Train Epoch: 19 Loss: 0.000127 Correct: 9962 ACC: 0.9962\n",
            "Test Loss: 0.000477 Correct: 57427, ACC : 0.9571166666666666\n",
            "Train Epoch: 20 Loss: 0.000406 Correct: 9969 ACC: 0.9969\n",
            "Test Loss: 0.001047 Correct: 57423, ACC : 0.95705\n",
            "Train Epoch: 21 Loss: 0.000211 Correct: 9979 ACC: 0.9979\n",
            "Test Loss: 0.000882 Correct: 57487, ACC : 0.9581166666666666\n",
            "Train Epoch: 22 Loss: 0.000739 Correct: 9971 ACC: 0.9971\n",
            "Test Loss: 0.000726 Correct: 57464, ACC : 0.9577333333333333\n",
            "Train Epoch: 23 Loss: 0.000827 Correct: 9971 ACC: 0.9971\n",
            "Test Loss: 0.000387 Correct: 57458, ACC : 0.9576333333333333\n",
            "Train Epoch: 24 Loss: 0.001591 Correct: 9989 ACC: 0.9989\n",
            "Test Loss: 0.000438 Correct: 57502, ACC : 0.9583666666666667\n",
            "Train Epoch: 25 Loss: 0.000096 Correct: 9988 ACC: 0.9988\n",
            "Test Loss: 0.000554 Correct: 57498, ACC : 0.9583\n",
            "Train Epoch: 26 Loss: 0.000936 Correct: 9978 ACC: 0.9978\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUanf9aHfM1z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}